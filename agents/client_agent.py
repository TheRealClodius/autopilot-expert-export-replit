"""
Client Agent - Generates user-friendly responses with personality.
Refactored to use state stack from orchestrator instead of direct memory access.
"""

import logging
from typing import Dict, Any, Optional, List

from utils.gemini_client import GeminiClient

logger = logging.getLogger(__name__)

class ClientAgent:
    """
    Client-facing agent responsible for generating user-friendly responses.
    Uses complete state stack from orchestrator for personality-driven formatting.
    """
    
    def __init__(self):
        self.gemini_client = GeminiClient()
        
    async def generate_response(self, state_stack: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Generate a persona-based response using complete state stack from orchestrator.
        
        Args:
            state_stack: Complete context state containing:
                - current_query: Current user query details
                - recent_history: Last 10 messages organized by user/agent
                - long_conversation_summary: Summary of older conversation
                - orchestrator_results: Vector search results and insights
                - metadata: Thread and conversation metadata
            
        Returns:
            Dictionary containing response text and suggestions
        """
        try:
            logger.info("Client Agent generating response from state stack...")
            
            # Prepare system prompt for persona-based response
            system_prompt = self._get_persona_prompt()
            
            # Prepare user prompt with complete state stack
            user_prompt = self._format_state_stack_for_response(state_stack)
            
            # Generate response using Gemini Flash (faster model)
            response = await self.gemini_client.generate_response(
                system_prompt,
                user_prompt,
                model="gemini-2.5-flash",
                max_tokens=500
            )
            
            if response:
                # Post-process response
                formatted_response = self._post_process_response(response, state_stack)
                
                # Generate suggestions based on the state stack
                suggestions = await self._generate_suggestions(state_stack, formatted_response)
                
                logger.info(f"Successfully generated client response with {len(suggestions)} suggestions")
                return {
                    "text": formatted_response,
                    "suggestions": suggestions
                }
            
            logger.warning("No response generated by Client Agent")
            return None
            
        except Exception as e:
            logger.error(f"Error generating client response: {e}")
            return None
    
    def _get_persona_prompt(self) -> str:
        """
        Get the persona-based system prompt for the client agent.
        
        Returns:
            System prompt with persona instructions
        """
        # Load prompt from centralized prompt loader
        from utils.prompt_loader import get_client_agent_prompt
        return get_client_agent_prompt()
    
    def _format_state_stack_for_response(self, state_stack: Dict[str, Any]) -> str:
        """
        Format the complete state stack for response generation.
        
        Args:
            state_stack: Complete context state from orchestrator
            
        Returns:
            Formatted prompt for response generation
        """
        prompt_parts = []
        
        # Current query context
        current_query = state_stack.get("current_query", {})
        prompt_parts.append(f"User Question: {current_query.get('text', '')}")
        prompt_parts.append(f"Asked by: {current_query.get('user', 'Unknown')} in {current_query.get('channel', 'Unknown')}")
        
        # Long conversation summary (if exists)
        long_summary = state_stack.get("long_conversation_summary", {})
        if long_summary.get("has_long_history"):
            prompt_parts.append("=== CONVERSATION CONTEXT ===")
            if long_summary.get("summary"):
                prompt_parts.append(f"Conversation Summary: {long_summary['summary']}")
        
        # Recent conversation history
        recent_history = state_stack.get("recent_history", {})
        user_queries = recent_history.get("user_queries", [])
        agent_responses = recent_history.get("agent_responses", [])
        
        if user_queries or agent_responses:
            prompt_parts.append("=== RECENT CONVERSATION HISTORY ===")
            
            # Combine and sort by timestamp for chronological order
            all_messages = []
            for query in user_queries:
                all_messages.append({
                    "type": "user", 
                    "text": query.get("text", ""), 
                    "user": query.get("user", ""), 
                    "timestamp": query.get("timestamp", "")
                })
            for response in agent_responses:
                all_messages.append({
                    "type": "agent", 
                    "text": response.get("text", ""), 
                    "timestamp": response.get("timestamp", "")
                })
            
            # Sort by timestamp and show recent ones
            all_messages.sort(key=lambda x: x.get("timestamp", ""))
            for msg in all_messages[-5:]:  # Last 5 messages
                if msg["type"] == "user":
                    prompt_parts.append(f"{msg['user']}: {msg['text']}")
                else:
                    prompt_parts.append(f"Assistant: {msg['text']}")
        
        # Orchestrator results and insights
        orchestrator_results = state_stack.get("orchestrator_results", {})
        
        # Add orchestrator insights
        if orchestrator_results.get("orchestrator_insights"):
            prompt_parts.append("=== ORCHESTRATOR ANALYSIS ===")
            prompt_parts.append(orchestrator_results["orchestrator_insights"])
        
        # Add vector search results
        vector_results = orchestrator_results.get("vector_search_results", [])
        if vector_results:
            prompt_parts.append("=== RELEVANT INFORMATION FROM KNOWLEDGE BASE ===")
            for i, result in enumerate(vector_results[:5], 1):  # Limit to top 5
                content = result.get("content", "")
                score = result.get("score", 0)
                metadata = result.get("metadata", {})
                
                source_info = ""
                if metadata.get("source"):
                    source_info = f" (Source: {metadata['source']})"
                if metadata.get("timestamp"):
                    source_info += f" (Date: {metadata['timestamp']})"
                
                prompt_parts.append(f"{i}. {content}{source_info}")
        
        # Add response approach guidance
        if orchestrator_results.get("response_approach"):
            prompt_parts.append(f"Response Approach: {orchestrator_results['response_approach']}")
        
        # Join all parts
        return "\n\n".join(prompt_parts)
    
    def _post_process_response(self, response: str, state_stack: Dict[str, Any]) -> str:
        """
        Post-process the generated response for better formatting.
        
        Args:
            response: Raw response from Gemini
            state_stack: State stack for context
            
        Returns:
            Formatted response
        """
        try:
            # Basic cleanup
            clean_response = response.strip()
            
            # Ensure response doesn't exceed reasonable length for Slack
            if len(clean_response) > 2000:
                clean_response = clean_response[:1950] + "...\n\n*[Response truncated for readability]*"
            
            return clean_response
            
        except Exception as e:
            logger.error(f"Error post-processing response: {e}")
            return response
    
    async def _generate_suggestions(self, state_stack: Dict[str, Any], response: str) -> List[str]:
        """
        Generate contextual suggestions based on state stack and response.
        
        Args:
            state_stack: Complete state stack
            response: Generated response text
            
        Returns:
            List of suggestion strings
        """
        try:
            current_query = state_stack.get("current_query", {})
            
            # Generate contextual suggestions using Gemini Flash
            suggestion_prompt = f"""
Based on this conversation context and response, generate 3-5 helpful follow-up suggestions for the user.

User Query: {current_query.get('text', '')}
Assistant Response: {response[:200]}...

Generate practical, specific suggestions that would help the user:
1. Learn more about the topic
2. Take next steps
3. Explore related features
4. Get additional help

Return suggestions as a simple list, one per line, starting with "-".
Keep each suggestion under 50 characters.
Focus on Autopilot and UiPath related topics.
"""
            
            suggestions_response = await self.gemini_client.generate_response(
                "You are a helpful assistant generating follow-up suggestions.",
                suggestion_prompt,
                model="gemini-2.5-flash",
                max_tokens=200
            )
            
            if suggestions_response:
                # Parse suggestions from response
                suggestions = []
                for line in suggestions_response.split('\n'):
                    line = line.strip()
                    if line.startswith('-') or line.startswith('â€¢'):
                        suggestion = line[1:].strip()
                        if suggestion and len(suggestion) <= 50:
                            suggestions.append(suggestion)
                
                return suggestions[:5]  # Max 5 suggestions
            
            # Fallback suggestions
            return [
                "Tell me more about this",
                "How does this work?",
                "Show me an example",
                "What are the next steps?"
            ]
            
        except Exception as e:
            logger.error(f"Error generating suggestions: {e}")
            return [
                "Tell me more",
                "How does this work?",
                "Show me an example"
            ]