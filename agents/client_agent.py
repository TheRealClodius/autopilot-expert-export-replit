"""
Client Agent - Generates persona-based responses using Gemini 2.5 Flash.
Formats responses for the end user based on gathered information.
"""

import logging
from typing import Dict, Any, Optional, List

from utils.gemini_client import GeminiClient
from models.schemas import ProcessedMessage

logger = logging.getLogger(__name__)

class ClientAgent:
    """
    Client-facing agent responsible for generating user-friendly responses.
    Uses Gemini 2.5 Flash for fast, persona-based response generation.
    """
    
    def __init__(self):
        self.gemini_client = GeminiClient()
        
    async def generate_response(
        self, 
        message: ProcessedMessage, 
        gathered_info: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Generate a persona-based response using gathered information.
        
        Args:
            message: Original processed message
            gathered_info: Information gathered by Orchestrator
            context: Additional context from execution plan
            
        Returns:
            Dictionary containing response text and suggestions
        """
        try:
            logger.info("Client Agent generating response...")
            
            # Prepare system prompt for persona-based response
            system_prompt = self._get_persona_prompt(message)
            
            # Prepare user prompt with gathered information
            user_prompt = self._format_information_for_response(
                message, 
                gathered_info, 
                context
            )
            
            # Generate response using Gemini Flash (faster model)
            response = await self.gemini_client.generate_response(
                system_prompt,
                user_prompt,
                model="gemini-2.5-flash",
                max_tokens=500
            )
            
            if response:
                # Post-process response
                formatted_response = self._post_process_response(response, message)
                
                # Generate suggestions based on the context
                suggestions = await self._generate_suggestions(message, gathered_info, context, formatted_response)
                
                logger.info(f"Successfully generated client response with {len(suggestions)} suggestions")
                return {
                    "text": formatted_response,
                    "suggestions": suggestions
                }
            
            logger.warning("No response generated by Client Agent")
            return None
            
        except Exception as e:
            logger.error(f"Error generating client response: {e}")
            return None
    
    def _get_persona_prompt(self, message: ProcessedMessage) -> str:
        """
        Get the appropriate persona prompt based on message context.
        
        Args:
            message: Processed message for context
            
        Returns:
            System prompt defining the AI persona
        """
        # Load prompt from centralized prompt loader
        from utils.prompt_loader import get_client_agent_prompt
        return get_client_agent_prompt()
    
    def _format_information_for_response(
        self, 
        message: ProcessedMessage,
        gathered_info: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """
        Format the gathered information for response generation.
        
        Args:
            message: Original processed message
            gathered_info: Information from vector search and graph queries
            context: Additional context
            
        Returns:
            Formatted prompt for response generation
        """
        prompt_parts = []
        
        # Add user query
        prompt_parts.append(f"User Question: {message.text}")
        
        # Add user context
        prompt_parts.append(f"Asked by: {message.user_name} in {message.channel_name}")
        
        # Add thread context if available
        if message.thread_context:
            prompt_parts.append(f"Thread Context:\n{message.thread_context}")
        
        # Add vector search results
        vector_results = gathered_info.get("vector_results", [])
        if vector_results:
            prompt_parts.append("=== RELEVANT INFORMATION FROM KNOWLEDGE BASE ===")
            for i, result in enumerate(vector_results[:5], 1):  # Limit to top 5
                content = result.get("content", "")
                score = result.get("score", 0)
                metadata = result.get("metadata", {})
                
                source_info = ""
                if metadata.get("source"):
                    source_info = f" (Source: {metadata['source']})"
                if metadata.get("timestamp"):
                    source_info += f" (Date: {metadata['timestamp']})"
                
                prompt_parts.append(f"{i}. {content}{source_info}")
        
        # Add graph query results
        graph_results = gathered_info.get("graph_results", [])
        if graph_results:
            prompt_parts.append("=== RELATIONSHIPS AND DEPENDENCIES ===")
            for i, result in enumerate(graph_results[:3], 1):  # Limit to top 3
                prompt_parts.append(f"{i}. {result}")
        
        # Add execution context
        if context.get("analysis"):
            prompt_parts.append(f"Analysis Context: {context['analysis']}")
        
        # Add instructions
        prompt_parts.append("""
=== RESPONSE INSTRUCTIONS ===
Based on the above information, provide a helpful response to the user's question.

Guidelines:
- Use the gathered information to provide specific, accurate answers
- If you found relevant information, cite it naturally in your response
- If information is incomplete, acknowledge what you found and what might be missing
- Suggest next steps or additional resources when appropriate
- Format your response clearly with bullet points or sections if needed
- Keep the tone professional but friendly

If you don't have enough information to fully answer the question, say so honestly and suggest how the user might get the information they need.
""")
        
        return "\n\n".join(prompt_parts)
    
    def _post_process_response(self, response: str, message: ProcessedMessage) -> str:
        """
        Post-process the generated response for final formatting.
        
        Args:
            response: Raw response from Gemini
            message: Original message for context
            
        Returns:
            Post-processed response
        """
        try:
            # Remove any unwanted prefixes or suffixes
            response = response.strip()
            
            # Ensure response doesn't exceed reasonable length for Slack
            if len(response) > 2000:
                # Truncate and add continuation message
                response = response[:1900] + "\n\n... (response truncated for brevity)"
            
            # No automatic footers - let the prompts handle conversational flow naturally
            
            return response
            
        except Exception as e:
            logger.error(f"Error post-processing response: {e}")
            return response
    
    async def _generate_suggestions(
        self, 
        message: ProcessedMessage,
        gathered_info: Dict[str, Any],
        context: Dict[str, Any],
        response_text: str
    ) -> List[str]:
        """
        Generate contextual suggestions for Slack AI agent mode.
        
        Args:
            message: Original processed message
            gathered_info: Information gathered by tools
            context: Execution context
            response_text: Generated response text
            
        Returns:
            List of suggestion strings
        """
        try:
            # Generate smart suggestions based on context
            suggestion_prompt = f"""
Based on this Autopilot expert conversation, generate 3-5 helpful follow-up questions or actions the user might want to explore next.

User's Original Question: {message.text}
My Response: {response_text}
Context: {message.channel_name} channel

Generate practical suggestions that would help the user:
1. Learn more about Autopilot features
2. Get specific implementation guidance  
3. Troubleshoot common issues
4. Explore related topics

Format as a simple list, one suggestion per line. Keep each under 75 characters.
Focus on actionable next steps, not generic questions.

Example format:
- How to set up Autopilot triggers?
- Best practices for AI Builder models
- Common deployment issues and fixes
"""

            suggestions_response = await self.gemini_client.generate_response(
                "You are an expert at generating helpful follow-up suggestions for Autopilot conversations.",
                suggestion_prompt,
                model="gemini-2.5-flash",
                max_tokens=200
            )
            
            if suggestions_response:
                # Parse suggestions from response
                suggestions = []
                lines = suggestions_response.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if line and (line.startswith('-') or line.startswith('â€¢')):
                        suggestion = line[1:].strip()
                        if suggestion and len(suggestion) <= 75:
                            suggestions.append(suggestion)
                
                # Fallback suggestions if parsing fails or no suggestions generated
                if not suggestions:
                    suggestions = self._get_fallback_suggestions(message, context)
                
                return suggestions[:5]  # Limit to 5 suggestions
            
            # Return fallback suggestions if generation fails
            return self._get_fallback_suggestions(message, context)
            
        except Exception as e:
            logger.error(f"Error generating suggestions: {e}")
            return self._get_fallback_suggestions(message, context)
    
    def _get_fallback_suggestions(self, message: ProcessedMessage, context: Dict[str, Any]) -> List[str]:
        """Get fallback suggestions when generation fails."""
        if message.is_dm:
            return [
                "How to create Autopilot actions?",
                "Best practices for AI Builder",
                "Troubleshooting automation issues"
            ]
        else:
            return [
                "Show me Autopilot examples",
                "Help with process automation",
                "AI Builder model tips"
            ]
