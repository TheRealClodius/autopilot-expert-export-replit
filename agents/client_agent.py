"""
Client Agent - Generates persona-based responses using Gemini 2.5 Flash.
Formats responses for the end user based on gathered information.
"""

import logging
from typing import Dict, Any, Optional

from utils.gemini_client import GeminiClient
from models.schemas import ProcessedMessage

logger = logging.getLogger(__name__)

class ClientAgent:
    """
    Client-facing agent responsible for generating user-friendly responses.
    Uses Gemini 2.5 Flash for fast, persona-based response generation.
    """
    
    def __init__(self):
        self.gemini_client = GeminiClient()
        
    async def generate_response(
        self, 
        message: ProcessedMessage, 
        gathered_info: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Optional[str]:
        """
        Generate a persona-based response using gathered information.
        
        Args:
            message: Original processed message
            gathered_info: Information gathered by Orchestrator
            context: Additional context from execution plan
            
        Returns:
            Generated response text
        """
        try:
            logger.info("Client Agent generating response...")
            
            # Prepare system prompt for persona-based response
            system_prompt = self._get_persona_prompt(message)
            
            # Prepare user prompt with gathered information
            user_prompt = self._format_information_for_response(
                message, 
                gathered_info, 
                context
            )
            
            # Generate response using Gemini Flash (faster model)
            response = await self.gemini_client.generate_response(
                system_prompt,
                user_prompt,
                model="gemini-2.5-flash",
                max_tokens=500
            )
            
            if response:
                # Post-process response
                formatted_response = self._post_process_response(response, message)
                logger.info("Successfully generated client response")
                return formatted_response
            
            logger.warning("No response generated by Client Agent")
            return None
            
        except Exception as e:
            logger.error(f"Error generating client response: {e}")
            return None
    
    def _get_persona_prompt(self, message: ProcessedMessage) -> str:
        """
        Get the appropriate persona prompt based on message context.
        
        Args:
            message: Processed message for context
            
        Returns:
            System prompt defining the AI persona
        """
        base_persona = """You are "Autopilot Expert", an AI assistant specializing in helping teams navigate and understand their organizational knowledge.

Your personality:
- Professional yet approachable
- Concise but comprehensive
- Helpful and solution-oriented
- Knowledgeable about projects, people, and processes

Your communication style:
- Use clear, direct language
- Provide specific, actionable information
- Include relevant context and details
- Format responses for easy reading
- Use bullet points or lists when appropriate

Your capabilities:
- Access to organizational knowledge base
- Understanding of project relationships and dependencies
- Awareness of team members and their roles
- Ability to find latest updates and information

"""
        
        # Customize persona based on context
        if message.is_dm:
            return base_persona + """
Since this is a direct message, you can be more conversational and detailed in your response.
Feel free to ask follow-up questions if you need clarification.
"""
        else:
            return base_persona + """
Since this is in a channel/thread, keep your response focused and concise.
Provide the key information without being overly verbose.
"""
    
    def _format_information_for_response(
        self, 
        message: ProcessedMessage,
        gathered_info: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """
        Format the gathered information for response generation.
        
        Args:
            message: Original processed message
            gathered_info: Information from vector search and graph queries
            context: Additional context
            
        Returns:
            Formatted prompt for response generation
        """
        prompt_parts = []
        
        # Add user query
        prompt_parts.append(f"User Question: {message.text}")
        
        # Add user context
        prompt_parts.append(f"Asked by: {message.user_name} in {message.channel_name}")
        
        # Add thread context if available
        if message.thread_context:
            prompt_parts.append(f"Thread Context:\n{message.thread_context}")
        
        # Add vector search results
        vector_results = gathered_info.get("vector_results", [])
        if vector_results:
            prompt_parts.append("=== RELEVANT INFORMATION FROM KNOWLEDGE BASE ===")
            for i, result in enumerate(vector_results[:5], 1):  # Limit to top 5
                content = result.get("content", "")
                score = result.get("score", 0)
                metadata = result.get("metadata", {})
                
                source_info = ""
                if metadata.get("source"):
                    source_info = f" (Source: {metadata['source']})"
                if metadata.get("timestamp"):
                    source_info += f" (Date: {metadata['timestamp']})"
                
                prompt_parts.append(f"{i}. {content}{source_info}")
        
        # Add graph query results
        graph_results = gathered_info.get("graph_results", [])
        if graph_results:
            prompt_parts.append("=== RELATIONSHIPS AND DEPENDENCIES ===")
            for i, result in enumerate(graph_results[:3], 1):  # Limit to top 3
                prompt_parts.append(f"{i}. {result}")
        
        # Add execution context
        if context.get("analysis"):
            prompt_parts.append(f"Analysis Context: {context['analysis']}")
        
        # Add instructions
        prompt_parts.append("""
=== RESPONSE INSTRUCTIONS ===
Based on the above information, provide a helpful response to the user's question.

Guidelines:
- Use the gathered information to provide specific, accurate answers
- If you found relevant information, cite it naturally in your response
- If information is incomplete, acknowledge what you found and what might be missing
- Suggest next steps or additional resources when appropriate
- Format your response clearly with bullet points or sections if needed
- Keep the tone professional but friendly

If you don't have enough information to fully answer the question, say so honestly and suggest how the user might get the information they need.
""")
        
        return "\n\n".join(prompt_parts)
    
    def _post_process_response(self, response: str, message: ProcessedMessage) -> str:
        """
        Post-process the generated response for final formatting.
        
        Args:
            response: Raw response from Gemini
            message: Original message for context
            
        Returns:
            Post-processed response
        """
        try:
            # Remove any unwanted prefixes or suffixes
            response = response.strip()
            
            # Ensure response doesn't exceed reasonable length for Slack
            if len(response) > 2000:
                # Truncate and add continuation message
                response = response[:1900] + "\n\n... (response truncated for brevity)"
            
            # Add helpful footer for DMs
            if message.is_dm and "I don't have" not in response and "I couldn't find" not in response:
                response += "\n\nNeed more details? Feel free to ask follow-up questions! ðŸ¤–"
            
            return response
            
        except Exception as e:
            logger.error(f"Error post-processing response: {e}")
            return response
