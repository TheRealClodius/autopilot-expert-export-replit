#!/usr/bin/env python3
"""
Test the complete "Autopilot for Everyone" summary generation flow
with the MCP parameter fix applied.

This test replicates the exact user scenario from the Slack screenshot:
"Can you create a summary of the autopilot for everyone project based on
all confluence pages you can find about it?"
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from agents.orchestrator_agent import OrchestratorAgent
from agents.client_agent import ClientAgent
from models.schemas import ProcessedMessage
from services.memory_service import MemoryService
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_autopilot_summary():
    """Test complete flow for Autopilot for Everyone summary generation"""
    print("🚀 TESTING COMPLETE AUTOPILOT SUMMARY GENERATION FLOW")
    print("=" * 60)
    
    # Initialize services
    memory_service = MemoryService()
    orchestrator = OrchestratorAgent(memory_service)
    client_agent = ClientAgent()
    
    # Create the exact message from the user scenario
    test_message = ProcessedMessage(
        channel_id="C087QKECFKQ",
        channel_name="general",
        user_id="U_ANDREI_CLODIUS",
        text="Can you create a summary of the autopilot for everyone project based on all confluence pages you can find about it?",
        message_ts="1751182507.951000",
        thread_ts=None,
        user_name="andrei.clodius",
        user_first_name="Andrei",
        user_display_name="Andrei Clodius",
        user_title="Product Manager",
        user_department="Product"
    )
    
    print(f"📝 Query: {test_message.text}")
    print(f"👤 User: {test_message.user_first_name} ({test_message.user_title})")
    print("\n" + "=" * 60)
    
    try:
        # Step 1: Orchestrator Analysis
        print("🧠 Step 1: Orchestrator Analysis")
        print("-" * 30)
        
        execution_plan = await orchestrator._analyze_query_and_plan(test_message)
        
        if not execution_plan:
            print("❌ FAILED: No execution plan returned from orchestrator")
            return False
        
        print(f"✅ Analysis: {execution_plan.get('analysis', 'No analysis')[:100]}...")
        print(f"✅ Tools needed: {execution_plan.get('tools_needed', [])}")
        
        # Step 1.5: Execute the plan to get gathered information
        print(f"\n⚡ Step 1.5: Executing Plan")
        print("-" * 30)
        
        gathered_info = await orchestrator._execute_plan(execution_plan, test_message)
        
        if not gathered_info:
            print("❌ FAILED: No gathered information from plan execution")
            return False
        
        # Check if Atlassian search was used
        atlassian_results = gathered_info.get("atlassian_results", [])
        if not atlassian_results:
            print("❌ FAILED: No Atlassian results found in gathered information")
            return False
        
        print(f"✅ Found {len(atlassian_results)} Confluence pages")
        
        # Step 2: Client Agent Response Generation
        print(f"\n✨ Step 2: Client Agent Response Generation")
        print("-" * 40)
        
        # Create state stack for client agent
        state_stack = {
            "query": test_message.text,
            "user_context": {
                "first_name": test_message.user_first_name,
                "title": test_message.user_title,
                "department": test_message.user_department
            },
            "conversation_history": {"recent_exchanges": []},
            "orchestrator_analysis": {
                "intent": execution_plan.get("analysis", ""),
                "tools_used": execution_plan.get("tools_needed", []),
                "atlassian_results": atlassian_results
            }
        }
        
        response = await client_agent.generate_response(state_stack)
        
        if not response:
            print("❌ FAILED: No response generated by client agent")
            return False
        
        # Display the response
        print("📋 GENERATED RESPONSE:")
        print("=" * 60)
        print(response.get("message", "No message content"))
        print("=" * 60)
        
        # Validate response quality
        response_text = response.get("message", "")
        validation_checks = {
            "Has content": len(response_text) > 100,
            "Mentions Autopilot": "autopilot" in response_text.lower(),
            "Has source links": "<https://" in response_text,
            "Professional tone": "summary" in response_text.lower() or "project" in response_text.lower()
        }
        
        print("\n🔍 RESPONSE VALIDATION:")
        print("-" * 30)
        for check, passed in validation_checks.items():
            status = "✅" if passed else "❌"
            print(f"{status} {check}: {passed}")
        
        all_passed = all(validation_checks.values())
        
        if all_passed:
            print(f"\n🎉 SUCCESS: Complete Autopilot summary generation working correctly!")
            print(f"✅ Orchestrator correctly routed to Confluence search")
            print(f"✅ MCP integration returning real UiPath data")  
            print(f"✅ Client agent generating professional summary with source links")
            return True
        else:
            print(f"\n⚠️ PARTIAL SUCCESS: Flow working but response quality needs improvement")
            return False
            
    except Exception as e:
        print(f"\n❌ ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_autopilot_summary())